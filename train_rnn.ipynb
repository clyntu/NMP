{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a RNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Main script used for training.\"\"\"\n",
    "from tensorflow.keras.callbacks import TensorBoard, CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras.metrics\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from nmp import model as mod\n",
    "from nmp import dataset, ev_metrics\n",
    "from nmp.dataset import pyplot_piano_roll\n",
    "from nmp import plotter\n",
    "from pathlib import Path\n",
    "import time\n",
    "import math\n",
    "import pypianoroll\n",
    "from pypianoroll import Multitrack, Track\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "SAVE = 0\n",
    "NOTES = '-' + 'lstm'\n",
    "\n",
    "# P = Path(__file__).parent.absolute()\n",
    "P = Path(os.path.abspath(''))  # Compatible with Jupyter Notebook\n",
    "P2 = Path('S:\\datasets')  # Dataset path\n",
    "\n",
    "PLOTS = P / 'plots'  # Plots path\n",
    "FS = 24  # Sampling frequency. 10 Hz = 100 ms\n",
    "Q = 0  # Quantize?\n",
    "st = 10  # Past timesteps\n",
    "num_ts = 10  # Predicted timesteps\n",
    "DOWN = 12  # Downsampling factor\n",
    "\n",
    "# D = \"data/Piano-midi.de\"  # Dataset\n",
    "D = \"data/Nottingham\"  # Dataset\n",
    "# D = \"data/JSB Chorales\"  # Dataset\n",
    "# D = \"data/MuseData\"  # Dataset\n",
    "\n",
    "LOW_LIM = 33  # A1\n",
    "HIGH_LIM = 97  # C7\n",
    "\n",
    "# LOW_LIM = 36  # C2\n",
    "# HIGH_LIM = 85  # C6\n",
    "\n",
    "# Complete 88-key keyboard\n",
    "# LOW_LIM = 21  # A0\n",
    "# HIGH_LIM = 109  # C8\n",
    "\n",
    "NUM_NOTES = HIGH_LIM - LOW_LIM\n",
    "CROP = [LOW_LIM, HIGH_LIM]  # Crop plots\n",
    "\n",
    "LOAD = 0\n",
    "TRANS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate list of MIDI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_list = [x for x in os.listdir(P / D / 'train') if x.endswith('.mid')]\n",
    "validation_list = [x for x in os.listdir(P / D / 'valid') if x.endswith('.mid')]\n",
    "\n",
    "# train_list = [train_list[x] for x in range(10)]\n",
    "# validation_list = [validation_list[x] for x in range(5)]\n",
    "\n",
    "print(\"\\nTrain list:  \", train_list)\n",
    "print(\"\\nValidation list:  \", validation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data from lists\n",
    "Training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "train = dataset.Dataset(train_list, P / D / 'train',  fs=FS, bl=0, quant=Q)\n",
    "validation = dataset.Dataset(validation_list, P / D / 'valid',  fs=FS, bl=0, quant=Q)\n",
    "\n",
    "train.build_rnn_dataset(\"training\", down=DOWN, low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "validation.build_rnn_dataset(\"validation\", down=DOWN, low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Done\")\n",
    "print(\"Loading time: %.2f\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.dataset[0].shape)\n",
    "print(train.dataset[1].shape)\n",
    "print(validation.dataset[0].shape)\n",
    "print(validation.dataset[1].shape)\n",
    "# print(test.dataset[0].shape)\n",
    "# print(test.dataset[1].shape)\n",
    "\n",
    "# pyplot_piano_roll(test.dataset[1][:, :NUM_NOTES], cmap=\"Oranges\",\n",
    "#                   low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# plt.title(\"Test target\")\n",
    "# plt.ylim(CROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train.dataset[0]))\n",
    "train_sequences = train_dataset.batch(seq_length, drop_remainder=True)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((validation.dataset[0]))\n",
    "valid_sequences = valid_dataset.batch(seq_length, drop_remainder=True)\n",
    "\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test.dataset[0]))\n",
    "# test_sequences = test_dataset.batch(10, drop_remainder=True)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:, :10*64]\n",
    "    target_text = chunk[:, 10*64:]\n",
    "#     tf.reshape(target_text, shape=[100, 640])\n",
    "#     input_text = chunk[:-1]\n",
    "#     target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "def split_input_target_base(chunk):\n",
    "    input_text = chunk[:,:10]\n",
    "    target_text = chunk[:,10:]\n",
    "#     input_text = chunk[:-1]\n",
    "#     target_text = chunk[:-1]\n",
    "    return input_text, target_text\n",
    "\n",
    "train_data = train_sequences.map(split_input_target)\n",
    "valid_data = valid_sequences.map(split_input_target)\n",
    "# test_data = test_sequences.map(split_input_target)\n",
    "# baseline_data = test_sequences.map(split_input_target_base)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_data = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "valid_data = valid_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# test_data = test_data.batch(1, drop_remainder=True)\n",
    "# baseline_data = baseline_data.batch(1, drop_remainder=True)\n",
    "print(\"Train: \", train_data)\n",
    "print(\"Valid: \", valid_data)\n",
    "# print(\"Test: \", test_data)\n",
    "# print(\"Baseline: \", baseline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_batch, label_batch in train_data.take(1):\n",
    "#     print(input_batch.shape)\n",
    "#     print(label_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piano rolls of training dataset\n",
    "Input and output piano rolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.figsize\"] = (20, 8)\n",
    "# pyplot_piano_roll(train.dataset[0][:, 0, :],\n",
    "#                   low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# plt.title(\"Train data\")\n",
    "# plt.ylim(CROP)\n",
    "# pyplot_piano_roll(train.dataset[1][:, :NUM_NOTES], cmap=\"Oranges\",\n",
    "#                   low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# plt.title(\"Train target\")\n",
    "# plt.ylim(CROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = BATCH_SIZE  # Batch size\n",
    "import importlib\n",
    "importlib.reload(mod)\n",
    "importlib.reload(dataset)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD:\n",
    "    model = load_model(filepath=model_path,\n",
    "                       custom_objects=None,\n",
    "                       compile=True)\n",
    "\n",
    "else:\n",
    "    model = mod.build_gru_model(NUM_NOTES, BS)\n",
    "    mod.compile_model(model, 'binary_crossentropy', 'adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "# Save logs\n",
    "logger = TensorBoard(log_dir=P / 'logs' / now.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                     write_graph=True, update_freq='epoch')\n",
    "\n",
    "csv_logger = CSVLogger(P / 'logs' / (now.strftime(\"%Y%m%d-%H%M%S\") + '-' +\n",
    "                       str(st) + '-' + str(num_ts) + '.csv'),\n",
    "                       separator=',', append=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model\n",
    "Try the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_example_batch, target_example_batch in train_data.take(1):\n",
    "#     example_batch_predictions = model(tf.cast(input_example_batch, tf.float32))\n",
    "#     print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = P / ('models/training_checkpoints/' + now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    period=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "Define batch size ```BS``` and number of ```epochs```\n",
    "\n",
    "#### fit generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit generator the model.\n",
    "# BS = 64  # Batch size\n",
    "# epochs = 20\n",
    "# start = time.time()\n",
    "# size_train = math.ceil(train.dataset[0].shape[0] / BS)\n",
    "# spe_train = size_train\n",
    "# size_valid = math.ceil(validation.dataset[0].shape[0] / BS)\n",
    "# spe_valid = size_valid\n",
    "# print(\"Train dataset shape: \", train.dataset[0].shape, \"\\n\")\n",
    "# print(\"Train dataset target shape: \", train.dataset[1].shape, \"\\n\")\n",
    "\n",
    "# # Fit generator. Data should be shuffled before fitting.\n",
    "# history = model.fit(dataset.generate((train.dataset[0], train.dataset[1]), trans=1), epochs=epochs,\n",
    "#           steps_per_epoch=spe_train,\n",
    "#           validation_data=dataset.generate((validation.dataset[0], validation.dataset[1])),\n",
    "#           validation_steps=spe_valid,\n",
    "#           callbacks=[logger, csv_logger])\n",
    "\n",
    "# end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model.\n",
    "BS = BATCH_SIZE  # Batch size\n",
    "epochs = 200\n",
    "start = time.time()\n",
    "\n",
    "# Normal fit. Auto-shuffles data.\n",
    "history = model.fit(train_data, validation_data=valid_data, epochs=epochs, shuffle=True,\n",
    "                    callbacks=[logger, csv_logger, checkpoint_callback])\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('rnn-' + D[5:] + '-loss-train.dat').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining time: \", (end-start), \"\\n\")\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "if SAVE:\n",
    "    hist['loss'].to_csv(('tables/rnn-' + D[5:] + '-loss-train' + NOTES + '.dat').lower(), sep=' ', header=None)\n",
    "    hist['val_loss'].to_csv(('tables/rnn-' + D[5:] + '-loss-valid' + NOTES + '.dat').lower(), sep=' ', header=None)\n",
    "\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss function of training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(5, 4))\n",
    "plt.plot(hist['val_loss'], '-', lw=3, c='tab:orange', label='Validation', ms=8, alpha=0.8)\n",
    "plt.plot(hist['loss'], '-', lw=1, c='tab:red', label='Train', ms=8, alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "# plt.xticks(range(epochs))\n",
    "plt.legend()\n",
    "plt.title('Loss: Binary cross-entropy')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "# plt.ylim([0.12, 0.16])\n",
    "# fig.savefig(PLOTS / 'compare-lstm-loss.eps', fmt='eps')\n",
    "print(\"Training time: \", (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to file\n",
    "Model can be loaded with:\n",
    "``` python\n",
    "load_model(filepath=str(folder_path), compile=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(str(P / 'models' / 'simpleRNN-nottingham-3') + '.h5', save_format='h5')\n",
    "# model.save(str(P / 'models' / 'lstm-z-de') + '.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation on train set:\")\n",
    "e_train = model.evaluate(train_data)\n",
    "\n",
    "print(\"\\nEvaluation on validation set:\")\n",
    "e_valid = model.evaluate(valid_data)\n",
    "\n",
    "# print(\"\\nEvaluation on test set:\")\n",
    "# e_test = model.evaluate(test_data)\n",
    "\n",
    "results = {out: e_train[i] for i, out in enumerate(model.metrics_names)}\n",
    "res = pd.DataFrame(list(results.items()), columns=['metric', 'train'])\n",
    "res = res.set_index('metric')\n",
    "\n",
    "results2 = {out: e_valid[i] for i, out in enumerate(model.metrics_names)}\n",
    "res2 = pd.DataFrame(list(results2.items()), columns=['metric', 'validation'])\n",
    "res2 = res2.set_index('metric')\n",
    "\n",
    "# results3 = {out: e_test[i] for i, out in enumerate(model.metrics_names)}\n",
    "# res3 = pd.DataFrame(list(results3.items()), columns=['metric', 'test'])\n",
    "# res3 = res3.set_index('metric')\n",
    "res3 = pd.DataFrame([])\n",
    "\n",
    "result = pd.concat([res, res2, res3], axis=1, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "Predictions from test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore last checkpoint\n",
    "\n",
    "Build again the model and restore the checkpoint with weights to use a different batch size for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mod.build_gru_model(NUM_NOTES, 1)\n",
    "mod.compile_model(model, 'binary_crossentropy', 'adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [x for x in os.listdir(P / D / 'test') if x.endswith('.mid')]\n",
    "# test_list = [test_list[x] for x in range(3)]\n",
    "test = dataset.Dataset(test_list, P / D / 'test',  fs=FS, bl=0, quant=Q)\n",
    "test.build_rnn_dataset(\"test\", down=DOWN, low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "test.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test.dataset[0]))\n",
    "test_sequences = test_dataset.batch(50, drop_remainder=True)\n",
    "test_data = test_sequences.map(split_input_target)\n",
    "test_data = test_data.batch(1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_bin = dataset.ranked_threshold(predictions[:, 0, :NUM_NOTES], 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot_piano_roll(predictions_bin, low_lim=LOW_LIM, high_lim=HIGH_LIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tests dataset (standard way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(dataset)\n",
    "# st = 10  # Past timesteps\n",
    "# num_ts = 10  # Predicted timesteps\n",
    "\n",
    "# test_list = [x for x in os.listdir(P / D / 'test') if x.endswith('.mid')]\n",
    "# # test_list = test_list[:20]\n",
    "# test = dataset.Dataset(test_list, P / D / 'test',  fs=FS, bl=0, quant=Q)\n",
    "# test.build_dataset(\"test\", step=st, t_step=num_ts, steps=st,\n",
    "#                    down=DOWN, low_lim=LOW_LIM, high_lim=HIGH_LIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test.dataset[0].shape)\n",
    "# pyplot_piano_roll(test.dataset[0][:, 0, :], low_lim=LOW_LIM, high_lim=HIGH_LIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaped = test.dataset[0].reshape(len(test.dataset[0]), 640)\n",
    "# reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(dataset)\n",
    "\n",
    "# rnn_predictions = []\n",
    "# for c, t in enumerate(reshaped):\n",
    "#     rnn_predictions.append(dataset.predict_rnn(t, model))\n",
    "#     if c % 1000 == 0:\n",
    "#         print(\"Processed timestep %d\" % c)\n",
    "# #     pyplot_piano_roll(predi, low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# #     pyplot_piano_roll(t, low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# #     break\n",
    "\n",
    "# rnn_predictions = np.array(rnn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot_piano_roll(rnn_predictions[:, 1, :], low_lim=LOW_LIM, high_lim=HIGH_LIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate AUC-ROC\n",
    "Reshape predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = rnn_predictions.reshape([len(rnn_predictions), 640])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build baseline\n",
    "# L = test.dataset[0].shape[0]\n",
    "# baseline = dataset.Dataset(test_list, P / D / 'test',  fs=FS, bl=1, quant=Q)\n",
    "# baseline.build_dataset(\"baseline\", step=st, t_step=num_ts, steps=st,\n",
    "#                        down=DOWN, low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Baseline shape: \", baseline.dataset[1].shape)\n",
    "# print(\"Test shape: \", test.dataset[1].shape)\n",
    "\n",
    "# pred_auc = ev_metrics.compute_auc(test.dataset[1][:L, :], predictions, NUM_NOTES)\n",
    "# base_auc = ev_metrics.compute_auc(test.dataset[1][:L, :], baseline.dataset[1][:L, :], NUM_NOTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2, axcb) = plt.subplots(1, 3, constrained_layout=True,\n",
    "#                                      figsize=(8, 8),\n",
    "#                                      gridspec_kw={'width_ratios':[1, 1, 0.08]})\n",
    "# g1 = sns.heatmap(pred_auc, vmin=0.5, vmax=1, cmap='copper', ax=ax1, cbar=False)\n",
    "# g1.set_ylabel('')\n",
    "# g1.set_xlabel('')\n",
    "# g1.set_yticklabels(g1.get_yticklabels(), rotation=0)\n",
    "# ax1.set_xlabel('Time (step)')\n",
    "# ax1.set_ylabel('Pitch')\n",
    "# ax1.set_title('AUC-ROC (prediction)')\n",
    "# g2 = sns.heatmap(base_auc, vmin=0.5, vmax=1, cmap='copper', ax=ax2, cbar_ax=axcb)\n",
    "# g2.set_ylabel('')\n",
    "# g2.set_xlabel('')\n",
    "# g2.set_yticks([])\n",
    "# ax2.set_xlabel('Time (step)')\n",
    "# ax2.set_title('AUC-ROC (baseline)')\n",
    "# ax1.get_shared_y_axes().join(ax1,ax2)\n",
    "# # plt.savefig(PLOTS / 'heat.eps', format='eps')\n",
    "# print(pred_auc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1 = 0\n",
    "# c2 = 64\n",
    "# fig, (ax1, ax2, axcb) = plt.subplots(1, 3, constrained_layout=True,\n",
    "#                                      figsize=(8, 6),\n",
    "#                                      gridspec_kw={'width_ratios':[1, 1, 0.08]})\n",
    "# g1 = sns.heatmap(pred_auc[c1:c2], vmin=0.5, vmax=1, cmap='gray', ax=ax1, cbar=False)\n",
    "# g1.set_ylabel('')\n",
    "# g1.set_xlabel('')\n",
    "# g1.set_yticklabels(g1.get_yticklabels(), rotation=0)\n",
    "# ax1.set_xlabel('Time (step)')\n",
    "# ax1.set_ylabel('Pitch')\n",
    "# ax1.set_title('AUC-ROC (crop) [prediction]')\n",
    "# g2 = sns.heatmap(base_auc[c1:c2], vmin=0.5, vmax=1, cmap='gray', ax=ax2, cbar_ax=axcb)\n",
    "# g2.set_ylabel('')\n",
    "# g2.set_xlabel('')\n",
    "# g2.set_yticks([])\n",
    "# ax2.set_xlabel('Time (step)')\n",
    "# ax2.set_title('AUC-ROC (crop) [baseline]')\n",
    "# ax1.get_shared_y_axes().join(ax1,ax2)\n",
    "# # plt.savefig(PLOTS / 'heat_crop.eps', format='eps')\n",
    "# print(pred_auc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(constrained_layout=True, figsize=(5, 4))\n",
    "\n",
    "# ax.plot(range(1, num_ts + 1), np.mean(pred_auc[c1:c2]), 'x', c='tab:blue', label='prediction', ms=10)\n",
    "# ax.plot(range(1, num_ts + 1), np.mean(base_auc[c1:c2]), 'o', c='tab:green', label='baseline ', ms=7)\n",
    "\n",
    "# ax.set_ylim([0.4, 1])\n",
    "# ax.set_ylim([0.4, 1])\n",
    "# ax.legend()\n",
    "# plt.title('Avg. AUC-ROC per predicted timestep')\n",
    "# plt.xlabel('Timestep')\n",
    "# # plt.xticks([0, 2, 4, 6, 8, 10])\n",
    "# plt.ylabel('ROC AUC')\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# name = 'auc' + str()\n",
    "# plt.grid()\n",
    "# # plt.savefig(PLOTS / 'rnn10.eps', format='eps')\n",
    "\n",
    "# print(\"Predict. mean value:\", np.mean(np.mean(pred_auc[c1:c2])))\n",
    "# print(\"Baseline mean value:\", np.mean(np.mean(base_auc[c1:c2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(pred_auc[c1:c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(base_auc[c1:c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "predauc = []\n",
    "baseauc = []\n",
    "\n",
    "merged_input = []\n",
    "merged_output = []\n",
    "merged_pred = []\n",
    "\n",
    "for input_batch, label_batch in test_data.take(-1):\n",
    "    predictions = model(tf.cast(input_batch, tf.float32))\n",
    "\n",
    "    # print(tf.squeeze(predictions, 0))\n",
    "    pred = np.array(tf.squeeze(predictions, 0))\n",
    "#     predictions_bin = dataset.ranked_threshold(pred, steps=1, how_many=5)\n",
    "\n",
    "    inp = np.array(tf.squeeze(input_batch, 0))\n",
    "    out = np.array(tf.squeeze(label_batch, 0))\n",
    "\n",
    "#     pyplot_piano_roll(out,\n",
    "#                       cmap=\"Greens\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "#     plt.title(\"Target\")\n",
    "#     plt.savefig(PLOTS / \"roll1.png\")\n",
    "\n",
    "#     pyplot_piano_roll(pred,\n",
    "#                       cmap=\"Purples\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "#     plt.title(\"Predictions\")\n",
    "#     plt.savefig(PLOTS / \"roll.png\")\n",
    "\n",
    "#     pyplot_piano_roll(inp,\n",
    "#                   cmap=\"Blues\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "#     plt.title(\"Baseline (repetition of the input)\")\n",
    "#     plt.savefig(PLOTS / \"roll3.png\")\n",
    "\n",
    "#     pred_auc = ev_metrics.compute_auc(out, pred, NUM_NOTES)\n",
    "#     base_auc = ev_metrics.compute_auc(out, inp, NUM_NOTES)\n",
    "#     print(pred_auc.shape)\n",
    "#     assert False\n",
    "#     pred_auc = 0\n",
    "#     base_auc = 0\n",
    "#     predauc.append(np.mean(np.mean(pred_auc)))\n",
    "#     baseauc.append(np.mean(np.mean(base_auc)))\n",
    "\n",
    "    \n",
    "    # Merged piano rolls to compute overall AUC.\n",
    "    merged_input.append(inp)\n",
    "    merged_output.append(out)\n",
    "    merged_pred.append(pred)\n",
    "\n",
    "merged_input = np.concatenate([x for x in merged_input])\n",
    "merged_output = np.concatenate([x for x in merged_output])\n",
    "merged_pred = np.concatenate([x for x in merged_pred])\n",
    "\n",
    "# merged_input = merged_input.reshape((len(merged_input), 10, 64))\n",
    "# merged_output = merged_output.reshape((len(merged_output), 10, 64))\n",
    "# merged_pred = merged_pred.reshape((len(merged_pred), 10, 64))\n",
    "\n",
    "print(merged_input.shape)\n",
    "\n",
    "baseline = copy.deepcopy(merged_input)\n",
    "for t in range(len(merged_input)):\n",
    "    for s in range(10):\n",
    "        baseline[t, 64*s:64*(s+1)] = merged_input[t, 640-64:640]\n",
    "# print(merged_output.shape)\n",
    "# print(merged_pred.shape)\n",
    "\n",
    "pred_auc = ev_metrics.compute_auc(merged_output, merged_pred, NUM_NOTES)\n",
    "base_auc = ev_metrics.compute_auc(merged_output, baseline, NUM_NOTES)\n",
    "\n",
    "# print(\"Pred AUC-ROC (mean of subsets): \", np.mean(predauc))\n",
    "# print(\"Base AUC-ROC:(mean of subsets): \", np.mean(baseauc))\n",
    "\n",
    "# print(\"Pred AUC-ROC (global): \", np.mean(np.mean((pred_auc_merged))))\n",
    "# print(\"Base AUC-ROC (global): \", np.mean(np.mean((base_auc_merged))))\n",
    "\n",
    "\n",
    "# # pyplot_piano_roll(test.dataset[1][:, :NUM_NOTES],\n",
    "# #                   cmap=\"Greens\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# # plt.title(\"Test target (ground truth)\")\n",
    "\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (13, 4)\n",
    "# pyplot_piano_roll(merged_output,\n",
    "#                   cmap=\"Greens\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# plt.title(\"Target (labels)\")\n",
    "# plt.ylim(CROP)\n",
    "# # plt.savefig(PLOTS / \"roll1.png\")\n",
    "# pyplot_piano_roll(merged_pred[:, :NUM_NOTES],\n",
    "#                   cmap=\"Blues\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# plt.title(\"Predictions\")\n",
    "# plt.ylim(CROP)\n",
    "# # plt.savefig(PLOTS / \"roll2.png\")\n",
    "# pyplot_piano_roll(merged_input[:, :NUM_NOTES],\n",
    "#                   cmap=\"Reds\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# plt.title(\"Baseline (equal to inputs)\")\n",
    "# plt.ylim(CROP)\n",
    "# # plt.savefig(PLOTS / \"roll3.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, axcb) = plt.subplots(1, 3, constrained_layout=True,\n",
    "                                     figsize=(8, 8),\n",
    "                                     gridspec_kw={'width_ratios':[1, 1, 0.08]})\n",
    "g1 = sns.heatmap(pred_auc, vmin=0.5, vmax=1, cmap='copper', ax=ax1, cbar=False)\n",
    "g1.set_ylabel('')\n",
    "g1.set_xlabel('')\n",
    "g1.set_yticklabels(g1.get_yticklabels(), rotation=0)\n",
    "ax1.set_xlabel('Time (step)')\n",
    "ax1.set_ylabel('Pitch')\n",
    "ax1.set_title('AUC-ROC (prediction)')\n",
    "g2 = sns.heatmap(base_auc, vmin=0.5, vmax=1, cmap='copper', ax=ax2, cbar_ax=axcb)\n",
    "g2.set_ylabel('')\n",
    "g2.set_xlabel('')\n",
    "g2.set_yticks([])\n",
    "ax2.set_xlabel('Time (step)')\n",
    "ax2.set_title('AUC-ROC (baseline)')\n",
    "ax1.get_shared_y_axes().join(ax1,ax2)\n",
    "# plt.savefig(PLOTS / 'heat.eps', format='eps')\n",
    "print(pred_auc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 0\n",
    "c2 = 64\n",
    "fig, (ax1, ax2, axcb) = plt.subplots(1, 3, constrained_layout=True,\n",
    "                                     figsize=(8, 6),\n",
    "                                     gridspec_kw={'width_ratios':[1, 1, 0.08]})\n",
    "g1 = sns.heatmap(pred_auc[c1:c2], vmin=0.5, vmax=1, cmap='gray', ax=ax1, cbar=False)\n",
    "g1.set_ylabel('')\n",
    "g1.set_xlabel('')\n",
    "g1.set_yticklabels(g1.get_yticklabels(), rotation=0)\n",
    "ax1.set_xlabel('Time (step)')\n",
    "ax1.set_ylabel('Pitch')\n",
    "ax1.set_title('AUC-ROC (crop) [prediction]')\n",
    "g2 = sns.heatmap(base_auc[c1:c2], vmin=0.5, vmax=1, cmap='gray', ax=ax2, cbar_ax=axcb)\n",
    "g2.set_ylabel('')\n",
    "g2.set_xlabel('')\n",
    "g2.set_yticks([])\n",
    "ax2.set_xlabel('Time (step)')\n",
    "ax2.set_title('AUC-ROC (crop) [baseline]')\n",
    "ax1.get_shared_y_axes().join(ax1,ax2)\n",
    "# plt.savefig(PLOTS / 'heat_crop.eps', format='eps')\n",
    "print(pred_auc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ts = 10\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(5, 4))\n",
    "\n",
    "ax.plot(range(1, num_ts + 1), np.mean(pred_auc[c1:c2]), 'x', c='tab:blue', label='prediction', ms=10)\n",
    "ax.plot(range(1, num_ts + 1), np.mean(base_auc[c1:c2]), 'o', c='tab:green', label='baseline ', ms=7)\n",
    "\n",
    "ax.set_ylim([0.4, 1])\n",
    "ax.set_ylim([0.4, 1])\n",
    "ax.legend()\n",
    "plt.title('Avg. AUC-ROC per predicted timestep')\n",
    "plt.xlabel('Timestep')\n",
    "# plt.xticks([0, 2, 4, 6, 8, 10])\n",
    "plt.ylabel('ROC AUC')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "name = 'auc' + str()\n",
    "plt.grid()\n",
    "# plt.savefig(PLOTS / 'compare-lstm-auc.eps', format='eps')\n",
    "\n",
    "print(\"Predict. mean value:\", np.mean(np.mean(pred_auc[c1:c2])))\n",
    "print(\"Baseline mean value:\", np.mean(np.mean(base_auc[c1:c2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    auc_df = pd.DataFrame(\n",
    "        {'pred': np.mean(pred_auc[c1:c2]),\n",
    "         'base': np.mean(base_auc[c1:c2])})\n",
    "    auc_df['pred'].to_csv(('tables/rnn-' + D[5:] + '-auc-pred' + NOTES + '.dat').lower(), sep=' ', header=None)\n",
    "    auc_df['base'].to_csv(('tables/rnn-' + D[5:] + '-auc-base' + NOTES + '.dat').lower(), sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = []\n",
    "# lab = []\n",
    "# cnt = 0\n",
    "# print(test_data)\n",
    "# for input_batch, label_batch in test_data.take(100):\n",
    "#     print(input_batch.shape)\n",
    "#     inp.append(np.array(tf.squeeze(input_batch, 0)))\n",
    "#     lab.append(np.array(tf.squeeze(label_batch, 0)))\n",
    "# inp = np.array(inp)\n",
    "# print(inp.shape)\n",
    "# pyplot_piano_roll(inp[:, 0, :], cmap=\"Blues\", low_lim=LOW_LIM, high_lim=HIGH_LIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_data(model, input_batch, size):\n",
    "#     generated = []\n",
    "# #     print(input_batch.shape)\n",
    "#     for i in range(size):\n",
    "# #         print(input_batch.shape)\n",
    "#         predictions = model(tf.cast(input_batch, tf.float32))\n",
    "#         pred = np.array(tf.squeeze(predictions, 0))\n",
    "#         predictions_bin = dataset.ranked_threshold(pred, steps=1, how_many=5)\n",
    "\n",
    "#         generated.append(np.array(predictions_bin[-1]))\n",
    "#         input_batch = tf.expand_dims([predictions_bin[-1]], 0)\n",
    "#     return np.array(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.reset_states()\n",
    "# predauc = []\n",
    "# baseauc = []\n",
    "\n",
    "# merged_input = []\n",
    "# merged_output = []\n",
    "# merged_pred = []\n",
    "\n",
    "# size = 10\n",
    "# import time\n",
    "\n",
    "\n",
    "# predizioni = []\n",
    "# a = 0\n",
    "# for input_batch, label_batch in test_data.take(-1):\n",
    "# #     print(input_batch.shape)\n",
    "# #     time.sleep(50)\n",
    "# #     print(a)\n",
    "#     a += 1\n",
    "\n",
    "#     generated = generate_data(model, input_batch, size)\n",
    "\n",
    "#     predizioni.append(generated)\n",
    "# #     print(\"OK\")\n",
    "# #     inp = np.array(tf.squeeze(input_batch, 0))\n",
    "# #     out = np.array(tf.squeeze(label_batch, 0))\n",
    "\n",
    "# #     pyplot_piano_roll(out,\n",
    "# #                       cmap=\"Greens\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# #     plt.title(\"Target\")\n",
    "# #     plt.savefig(PLOTS / \"roll1.png\")\n",
    "\n",
    "# #     pyplot_piano_roll(pred,\n",
    "# #                       cmap=\"Purples\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# #     plt.title(\"Predictions\")\n",
    "# #     plt.savefig(PLOTS / \"roll.png\")\n",
    "\n",
    "# #     pyplot_piano_roll(inp,\n",
    "# #                   cmap=\"Blues\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# #     plt.title(\"Baseline (repetition of the input)\")\n",
    "# #     plt.savefig(PLOTS / \"roll3.png\")\n",
    "\n",
    "# #     pred_auc = ev_metrics.compute_auc(out, pred, NUM_NOTES)\n",
    "# #     base_auc = ev_metrics.compute_auc(out, inp, NUM_NOTES)\n",
    "# #     predauc.append(np.mean(np.mean(pred_auc)))\n",
    "# #     baseauc.append(np.mean(np.mean(base_auc)))\n",
    "\n",
    "    \n",
    "#     # Merged piano rolls to compute overall AUC.\n",
    "# #     merged_input.append(inp)\n",
    "# #     merged_output.append(out)\n",
    "# #     merged_pred.append(pred)\n",
    "\n",
    "# # merged_input = np.concatenate([x for x in merged_input])\n",
    "# # merged_output = np.concatenate([x for x in merged_output])\n",
    "# # merged_pred = np.concatenate([x for x in merged_pred])\n",
    "\n",
    "# # pred_auc_merged = ev_metrics.compute_auc(merged_output, merged_pred, NUM_NOTES)\n",
    "# # base_auc_merged = ev_metrics.compute_auc(merged_output, merged_input, NUM_NOTES)\n",
    "\n",
    "# # print(\"Pred AUC-ROC (mean of subsets): \", np.mean(predauc))\n",
    "# # print(\"Base AUC-ROC:(mean of subsets): \", np.mean(baseauc))\n",
    "\n",
    "# # print(\"Pred AUC-ROC (global): \", np.mean(np.mean((pred_auc_merged))))\n",
    "# # print(\"Base AUC-ROC (global): \", np.mean(np.mean((base_auc_merged))))\n",
    "\n",
    "# predizioni = np.array(predizioni)\n",
    "# print(predizioni.shape)\n",
    "\n",
    "# # pyplot_piano_roll(test.dataset[1][:, :NUM_NOTES],\n",
    "# #                   cmap=\"Greens\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# # plt.title(\"Test target (ground truth)\")\n",
    "\n",
    "\n",
    "# # plt.rcParams[\"figure.figsize\"] = (13, 4)\n",
    "# # pyplot_piano_roll(merged_output,\n",
    "# #                   cmap=\"Greens\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# # plt.title(\"Target (labels)\")\n",
    "# # plt.ylim(CROP)\n",
    "# # plt.savefig(PLOTS / \"roll1.png\")\n",
    "# # pyplot_piano_roll(merged_pred[:, :NUM_NOTES],\n",
    "# #                   cmap=\"Blues\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# # plt.title(\"Predictions\")\n",
    "# # plt.ylim(CROP)\n",
    "# # plt.savefig(PLOTS / \"roll2.png\")\n",
    "# # pyplot_piano_roll(merged_input[:, :NUM_NOTES],\n",
    "# #                   cmap=\"Reds\", low_lim=LOW_LIM, high_lim=HIGH_LIM)\n",
    "# # plt.title(\"Baseline (equal to inputs)\")\n",
    "# # plt.ylim(CROP)\n",
    "# # plt.savefig(PLOTS / \"roll3.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     pyplot_piano_roll(predizioni[:, i, :], low_lim=LOW_LIM, high_lim=HIGH_LIM)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
